<html>
<HEAD><META HTTP-EQUIV="Pragma" CONTENT="no-cache"><META HTTP-EQUIV="Expires" CONTENT="-1">
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>Configuring huge pages for GT.M x86[-64] on Linux</title>
<link rel="stylesheet" href="gtm.css" type="text/css">
<meta name="generator" content="DocBook XSL Stylesheets V1.75.2">
<link rel="home" href="index.html" title="GT.M Administration and Operations Guide">
<link rel="up" href="ch03.html" title="Chapter 3. Basic Operations">
<link rel="prev" href="ch03s04.html" title="Starting GT.M">
<link rel="next" href="ch03s06.html" title="Configuring the Restriction facility">
<link rel="copyright" href="ln-idm45622523859200.html" title="Legal Notice">
</head>
<body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF">
<div class="navheader">
<table width="100%" summary="Navigation header">
<tr><th colspan="3" align="center">Configuring huge pages for GT.M x86[-64] on Linux</th></tr>
<tr>
<td width="20%" align="left">
<a accesskey="p" href="ch03s04.html">Prev</a> </td>
<th width="60%" align="center">Chapter 3. Basic Operations</th>
<td width="20%" align="right"> <a accesskey="n" href="ch03s06.html">Next</a>
</td>
</tr>
</table>
<hr>
</div>
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">GT.M Administration and Operations Guide</a></span> &gt; <span class="breadcrumb-link"><a href="ch03.html">Basic Operations</a></span> &gt; <span class="breadcrumb-node">Configuring huge pages for GT.M x86[-64] on Linux</span>
</div>
<div class="sect1">
<div class="titlepage"><div><div><h2 class="title" style="clear: both">
<a name="conf_huge_pgs"></a>Configuring huge pages for GT.M x86[-64] on Linux<a class="indexterm" name="idm45622522854624"></a><a class="indexterm" name="idm45622522825088"></a>
</h2></div></div></div>
<p>Huge pages are a Linux feature that may improve the performance of GT.M applications in production. Huge pages create a single page table entry for a large block (typically 2MiB) of memory in place of hundreds entries for many smaller (typically 4KiB) blocks. This reduction of memory used for page tables frees memory for other uses, such as file system caches, and increases the probability of TLB (translation lookaside buffer) matches, both of which can improve performance. The performance improvement related to reducing the page table size becomes evident when many processes share memory as they do for global buffers, journal buffers, and replication journal pools. Configuring huge pages on Linux for x86 or x86_64 CPU architectures help improve:</p>
<div class="orderedlist"><ol class="orderedlist" type="1">
<li class="listitem"><p><span class="bold"><strong>GT.M shared memory performance:</strong></span> When your GT.M database uses journaling, replication, and the BG access method.</p></li>
<li class="listitem"><p><span class="bold"><strong>GT.M process memory performance: </strong></span>For your process working space and dynamically linked code. </p></li>
</ol></div>
<div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note: Note">
<tr>
<td rowspan="2" align="center" valign="top" width="12pt"><img alt="[Note]" src="images/note.jpg"></td>
<th align="left">Note</th>
</tr>
<tr><td align="left" valign="top"><p>At this time, huge pages have no effect for MM databases; the text, data, or bss segments for each process; or for process stack.</p></td></tr>
</table></div>
<p>While FIS recommends you configure huge page for shared memory, you need to evaluate whether or not configuring huge page for process-private memory is appropriate for your application. Having insufficient huge pages available during certain commands (for example, a JOB command - see complete list below) can result in a process terminating with a SIGBUS error. This is a current limitation of Linux. Before you use huge pages for process private memory on production systems, FIS recommends that you perform appropriate peak load tests on your application and ensure that you have an adequate number of huge pages configured for your peak workloads or that your application is configured to perform robustly when processes terminate with SIGBUS errors. The following GT.M features fork processes and may generate SIGBUS errors when huge pages are not available-JOB, OPEN of a PIPE device, ZSYSTEM, interprocess signaling that requires the services of gtmsecshr when gtmsecshr is not already running, SPAWN commands in DSE, GDE, and LKE, argumentless MUPIP RUNDOWN, and replication-related MUPIP commands that start server processes and/or helper processes.</p>
<p>Consider the following example of a memory map report of a Source Server process running at peak load:</p>
<pre class="programlisting">$ pmap -d 18839
18839: /usr/lib/fis-gtm/V6.2-000_x86_64/mupip replicate -source -start -buffsize=1048576 -secondary=melbourne:1235 -log=/var/log/.fis-gtm/mal2mel.log -instsecondary=melbourne
Address   Kbytes Mode Offset   Device Mapping
--- lines removed for brevity -----
mapped: 61604K writeable/private: 3592K shared: 33532K
$</pre>
<p>Process id 18839 uses a large amount of shared memory (33535K) and can benefit from configuring huge pages for shared memory. Configuring huge pages for shared memory does not cause a SIGBUS error when a process does a fork. For information on configuring huge pages for shared memory, refer to "Using huge pages" and "Using huge pages for shared memory" sections. SIGBUS errors only occur when you configure huge pages for process private memory; these errors indicate you have not configured your system with an adequate number of huge pages. To prevent SIGBUS errors, you should perform peak load tests on your application to determine the number of required huge pages. For information on configuring huge pages for process private memory, refer to "Using huge pages" and "Using huge pages for process working space" sections.</p>
<p>As application response time can be deleteriously affected if processes and database shared memory segments are paged out, FIS recommends configuring systems for use in production with sufficient RAM so as to not require swap space or a swap file. While you must configure an adequate number of huge pages for your application needs as empirically determined by benchmarking / testing, and there is little downside to a generous configuration to ensure a buffer of huge pages available for workload spikes, an excessive allocation of huge pages may affect system throughput by reserving memory for huge pages that could otherwise be used by applications that cannot use huge pages.</p>
<div class="sect2">
<div class="titlepage"><div><div><h3 class="title">
<a name="huge_use"></a>Using huge pages</h3></div></div></div>
<div class="informaltable"><table border="1">
<colgroup>
<col>
<col>
</colgroup>
<thead><tr>
<th>
<p>Prerequisites </p>
</th>
<th>
<p>Notes</p>
</th>
</tr></thead>
<tbody>
<tr>
<td>
<p>A 32- or 64-bit x86 CPU running a Linux kernel with huge pages enabled.</p>
</td>
<td>
<p>All currently Supported Linux distributions appear to support huge pages; to confirm, use the command: <code class="code">grep hugetlbfs /proc/filesystems</code> which should report: <code class="code">nodev hugetlbfs</code> </p>
</td>
</tr>
<tr>
<td>
<p>Have sufficient number of huge pages available. </p>
</td>
<td>
<p>To reserve Huge Pages boot Linux with the hugepages=num_pages kernel boot parameter; or, shortly after bootup when unfragmented memory is still available, with the command: <code class="code">hugeadm --pool-pages-min DEFAULT:num_pages</code> </p>
<p>For subsequent on-demand allocation of Huge Pages, use: <code class="code">hugeadm --pool-pages-max DEFAULT:num_pages</code></p>
<p>These delayed (from boot) actions do not guarantee availability of the requested number of huge pages; however, they are safe as, if a sufficient number of huge pages are not available, Linux simply uses traditional sized pages. </p>
</td>
</tr>
</tbody>
</table></div>
<div class="sect3">
<div class="titlepage"><div><div><h4 class="title">
<a name="using_huge_pages"></a>Using huge pages for shared memory</h4></div></div></div>
<p><span class="bold"><strong>To use huge pages for shared memory (journal buffers, replication journal pool and global buffers):</strong></span></p>
<div class="itemizedlist"><ul class="itemizedlist" type="disc">
<li class="listitem">
<p>Permit GT.M processes to use huge pages for shared memory segments (where available, FIS recommends option 1 below; however not all file systems support extended attributes). Either: </p>
<div class="orderedlist"><ol class="orderedlist" type="1">
<li class="listitem">
<p>Set the CAP_IPC_LOCK capability needs for your mumps, mupip and dse processes with a command such as:</p>
<pre class="programlisting">setcap 'cap_ipc_lock+ep' $gtm_dist/mumps</pre>
<p>or </p>
</li>
<li class="listitem">
<p>Permit the group used by GT.M processes needs to use huge pages with the following command, which requires root priviliges:</p>
<pre class="programlisting">echo gid &gt;/proc/sys/vm/hugetlb_shm_group</pre>
</li>
</ol></div>
</li>
<li class="listitem"><p>Set the environment variable gtm_hugetlb_shm for each process to "yes". </p></li>
</ul></div>
</div>
<div class="sect3"><div class="titlepage"><div><div><h4 class="title">
<a name="using_huge_pages_for_ppm"></a>Using huge pages for GT.M process private memory</h4></div></div></div></div>
<p>Refer to the documentation of your Linux distribution for details. Other sources of information are: </p>
<div class="itemizedlist"><ul class="itemizedlist" type="disc">
<li class="listitem"><p><a class="ulink" href="http://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt" target="_top">http://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt</a></p></li>
<li class="listitem"><p> <a class="ulink" href="http://lwn.net/Articles/374424/" target="_top">http://lwn.net/Articles/374424/</a></p></li>
<li class="listitem"><p> <a class="ulink" href="http://www.ibm.com/developerworks/wikis/display/LinuxP/libhuge+short+and+simple" target="_top">http://www.ibm.com/developerworks/wikis/display/LinuxP/libhuge+short+and+simple</a></p></li>
</ul></div>
<div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note: Note">
<tr>
<td rowspan="2" align="center" valign="top" width="12pt"><img alt="[Note]" src="images/note.jpg"></td>
<th align="left">Note</th>
</tr>
<tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc">
<li class="listitem"><p>Since the memory allocated by Linux for shared memory segments mapped with huge pages is rounded up to the next multiple of huge pages, there is potentially unused memory in each such shared memory segment. You can therefore increase any or all of the number of global buffers, journal buffers, and lock space to make use of this otherwise unused space. You can make this determination by looking at the size of shared memory segments using ipcs. Contact FIS GT.M support for a sample program to help you automate the estimate.</p></li>
<li class="listitem"><p>Transparent huge pages may further improve virtual memory page table efficiency. Some Supported releases automatically set transparent_hugepages to "always"; others may require it to be set at or shortly after boot-up. Consult your Linux distribution's documentation.</p></li>
</ul></div></td></tr>
</table></div>
</div>
</div>
<div class="navfooter">
<hr>
<table width="100%" summary="Navigation footer">
<tr>
<td width="40%" align="left">
<a accesskey="p" href="ch03s04.html">Prev</a> </td>
<td width="20%" align="center"><a accesskey="u" href="ch03.html">Up</a></td>
<td width="40%" align="right"> <a accesskey="n" href="ch03s06.html">Next</a>
</td>
</tr>
<tr>
<td width="40%" align="left" valign="top">Starting GT.M  </td>
<td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td>
<td width="40%" align="right" valign="top"> Configuring the Restriction facility</td>
</tr>
</table>
</div>
</body>
</html>
